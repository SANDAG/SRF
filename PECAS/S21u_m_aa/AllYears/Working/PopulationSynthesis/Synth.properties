inputLocation=./
initialGenerationBasis=previous
maxIterations =  2E7
reportIterations= 1E6

# Simulated annealing parameters
coolingParameter=0
gofDifExponent=0.0

# in this version of the targets all the space have the same weight
samplesTable=samples_sandag
targetsTable=PopSynTargets

# This set of properties turn on the sample weights, if weightColumn is specified here then
# the program will also look for the file weightmultipliers.csv
# weightColumn=weight
# e.g. if sampleGeographyColumnName=taz then you can set up the 
# weightmultipliers.csv to have a higher probability of using samples
# from within the same TAZ.
# sampleGeographyColumnName=cma
# defaultSampleWeightMultiplier is only used if weightmultipliers are being used.  
# For example if
# you only want to use samples from within the same TAZ, set this to 0.0, but
# if you set your sample weights for the taz to 1.0 and you want a 1/10 probability
# of trying samples from outside the TAZ, set this to 0.1.
# in this run I lower the defaultSampleWeightMultiplier from 0.1 to 0.05
# defaultSampleWeightMultiplier=0.1

# Caching values is makes the program much faster, but also causes it to use more memory.
cacheValues=true

# Controlling how much time is spent on the "bad" rows, vs the rows that have already
# achieved a good fit.  Have to balance these too: if iterationsPerZonalLakeOfFit
# is too high the program can spend too much time on problem zones that will always
# have a bad fit due to inconsistency in the data, but if minIterationsPerRow is too 
# high the program can spend too much time looking for small improvements in zones
# where the fit is already quite good.
minIterationsPerRow=10
iterationsPerZonalLackOfFit=0

# Instance count penalty options, to ensure that each sample is used an appropriate
# number of times.  Instance count penalties are a lot easier to define
# than entropy and can acheive some of the same effect.  Instance count penalties use quite a bit of memory if the 
# sample table is large.  If the entropy weight is large, the instance 
# count penalties can be important to stop the model from pushing some 
# samples beyond the inflection point in the normal curve, where they no longer
# contribute to entropy and the search process can continue to abuse them to
# maximize entropy in other similar samples.


#the 3rd run with this feature:
#highInstancePenalty=10000 
#highInstanceCountColumn=max_sam

#lowInstancePenalty=100
#lowInstanceCountColumn=min_sam

# entropySDWeight=automatic sets entropy to fall within max/min instance count penalties

#entropyDistribution=Lognormal
#entropySDWeight=10.0
#entropySDWeight=1.0


# Specify weight for entropy calculation
# 
#entropyWeight=1E05


